---
title: "Sentiment Analysis: Llama Llama"
subtitle: "Comparing Llama with the Bing lexicon"
date: November 8, 2024
author: "Brian Calhoon"
title-block-banner: "#C9C9C9"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    css: ./styles.css
execute: 
  warning: false
  message: false
  error: false
editor: visual

---

```{r global_options, include=F, warning=F, message=F, error=F}
source(here::here("prep/prep.R"))

#library(tidytext)
library(ollamar)
library(mall)
library(remotes)
library(textdata)
library(tidytext)


```



# Why am I here?        `r fontawesome::fa("earlybirds", fill = "#FFB947", height = '2em', width = '2em')`

::::::: columns
:::: {.column width="20%"}
::: box-header
![](./humanslearning.png)
:::
::::

:::: {.column width="80%"}
::: box
::: box-header 

It might be because you are curious about sentiment analysis. This lesson will take an example dataframe that has a text field and compare the ability of Meta's LLama 3.2,  bing's lexicon, and Loughran-McDonald's lexicon at identifying positive and negative sentiments from short responses. This draws on the great work of Julia Silge and David Robinson [Text Mining with R](www.tidytextmining.com) for the lexicon-based sentiment analysis. For the Llama part of the analysis, the `ollama` and `mall` R packages do the heavy lifting with `mall` providing the tidy functions that are used to interact with the data. Thanks for learning with me today. Enjoy!
:::
::::
:::::::



# Comparing the different approaches to sentiment analysis

I am curious to test this out for a few reasons. First, I find sentiments an interesting way to categorize qualitative data. Second, I like the idea of running a large language model (LLM) locally. Third, this is an open-source large language model so the costs are minimal. Fourth, I have been curious how lexicon-based sentiment analysis compares to what an LLM can do. So, let's dive in. 


## A dataset

Here's a dataset that we can use. You can explore it in the table below. For our purposes we're mainly interested in the column `text_response`.

```{r}
df <- readxl::read_xlsx("C:/Users/Brian Calhoon/Documents/Shiny-sandbox/data/testdata2.xlsx")

DT::datatable(df, class = 'cell-border stripe', option = list(pageLength = 5), filter = "top")


```

## Sentiment analysis with bing and Loughran-McDonald


```{r message = F, warning = F, include = F}

bing <- tidytext::get_sentiments(lexicon = "bing")
loughran <- textdata::lexicon_loughran()


```

Sentiment lexicons are basically lists of words that have been coded with a sentiment value (positive, negative, or a number score). The bing lexicon has `r nrow(bing)` words and the Loughran-McDonald has `r nrow(loughran)` words. We can tokenize and identify the sentiment words and then quantify all positive words as 1 and negative words as -1 before computing an average sentiment score for each respondent.


```{r}

# data wrangling to get the bing sentiments object
bing_sents <- df |>
  dplyr::select(text_response) |> 
  dplyr::mutate(respondent = row_number()) |> 
  tidytext::unnest_tokens(word, text_response) |> 
  dplyr::inner_join(bing) |> 
  dplyr::mutate(value = case_when(sentiment == "positive" ~ 1
                               , sentiment == "negative" ~ -1)) |> 
      dplyr::group_by(respondent) |> 
      dplyr::summarize(score = mean(value))

# data wrangling to get the Loughran-McDonald sentiments object
loughran_sents <- df |>
  dplyr::select(text_response) |> 
  dplyr::mutate(respondent = row_number()) |> 
  tidytext::unnest_tokens(word, text_response) |> 
  dplyr::inner_join(loughran) |> 
  dplyr::mutate(value = case_when(sentiment == "positive" ~ 1
                               , sentiment == "negative" ~ -1)) |> 
      dplyr::group_by(respondent) |> 
      dplyr::summarize(score = mean(value))

```
:::

Then, we do a similar process using Llama, but we don't have to take an average score because the LLM provides its own rating of positive, negative, or neutral for each respondent. 

```{r message = F, warning = F}


mall::llm_use("ollama", "llama3.2", seed = 100, .cache = "_readme_cache")

llama_sents <- df |>
  dplyr::select(text_response) |> 
  dplyr::mutate(respondent = row_number()) |> 
  mall::llm_sentiment(col = text_response, options = c("positive",  "negative", "neutral")) |> 
  dplyr::mutate(value = case_when(.sentiment == "positive" ~ 1
                               , .sentiment == "negative" ~ -1
                               , .sentiment == "neutral" ~ 0)) 

```

## Comparing the outputs

To interpret the density plots for each model think of each point on the line representing that proportion of respondents (see the vertical axis). The Llama LLM identifies approximately a .58 proportion of respondents as being negative. This is more negative than the other two models, and in looking at the data, it appears to be more accurate as well.  

```{r}


markdown_text <- "The <span style = 'color:#00457C'><b>bing<b></span> and <span style = 'color:#89C266;'><b>Loughran-McDonald<b></span> sentiment lexicons <br> miss the nuance of the language that the <span style = 'color:#FF7C48;'><b>Llama<b></span> <br> sentiment analysis captures."

ggplot(bing_sents) +
  geom_vline(xintercept = 0, lwd = 2, color = "#7F7F7F", alpha = .3)+
  geom_density(aes(score), color = "#00457C", lwd = 1.5) +
  geom_density(data = loughran_sents, aes(score), color = "#89C266", lwd = 1.5) +
  geom_density(data = llama_sents, aes(value), color = "#FF7C48", lwd = 1.5) +
  geom_text(aes(x = .25, y = .5, label = "Neutral"), color = "#7F7F7F", vjust = .25, alpha = .3) +
  geom_segment(aes(x = .03, xend = .16, y = .45, yend = .49), color = "#7F7F7F", alpha = .3)+
  theme_minimal()+
  labs(title = "Comparison of sentiments with different methods", 
    subtitle = markdown_text,
    y = "Propotion of responses",
    x = "More negative                |                More positive") +
  theme(plot.title = element_text(size = 20, face = "bold", family = "Corbel"), 
    plot.subtitle = element_markdown(size = 16, family = "Corbel"),
    axis.title = element_text(family = "Corbel"),
    plot.title.position = "plot"
    , axis.text.x = element_blank())

```